{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HBPSHPO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPft6vuIl4Ul9cKwiid2lRd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/titapag/HBPSHPO/blob/main/HBPSHPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IgKsur-Lu0U"
      },
      "source": [
        "pip install --upgrade pyswarm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynDXprCC1QpU"
      },
      "source": [
        "pip install scikit-optimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA1tM0_kG7pb"
      },
      "source": [
        "from pyswarm import pso"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysHsG-Wn6pKW"
      },
      "source": [
        "from os import path\n",
        "import os\n",
        "import requests\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "import numpy\n",
        "import sys\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "from numpy import array\n",
        "from numpy.random import choice\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import statistics\n",
        "import pandas\n",
        "import math\n",
        "import csv\n",
        "import random\n",
        "from operator import add\n",
        "from scipy import stats\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt \n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlstLIxi1NAm"
      },
      "source": [
        "from skopt.space import Categorical, Real\n",
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpDHvM68Xl0C"
      },
      "source": [
        "def truncate(n, decimals=0):\n",
        "    multiplier = 10 ** decimals\n",
        "    return int(n * multiplier) / multiplier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlRgcejgoeqf"
      },
      "source": [
        "def hasLSTM(model):\n",
        "\tfirstlayer=model.layers[0].name[:4]\n",
        "\tif(firstlayer=='lstm'):\n",
        "\t\tanswer=True\n",
        "\telse:\n",
        "\t\tanswer=False\n",
        "\treturn answer\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6xR7zHBzYvz"
      },
      "source": [
        "def hasCONV1D(model):\n",
        "\tfirstlayer=model.layers[0].name[:6]\n",
        "\tif(firstlayer=='conv1d'):\n",
        "\t\tanswer=True\n",
        "\telse:\n",
        "\t\tanswer=False\n",
        "\treturn answer\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uSj8_-g9prV"
      },
      "source": [
        "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequences)):\n",
        "\t\tend_ix = i + n_steps_in\n",
        "\t\tout_end_ix = end_ix + n_steps_out\n",
        "\t\tif out_end_ix > len(sequences):\n",
        "\t\t\tbreak\n",
        "\t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn np.array(X), np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc7hyZI4JBHy"
      },
      "source": [
        "def metrics(model, X_test, y_test, scaler):\n",
        "\tprediction=model.predict(X_test)\t\n",
        "\tmse = mean_squared_error(y_test, prediction)\n",
        "\tmae = mean_absolute_error(y_test, prediction)\n",
        "\ty_test=scaler.inverse_transform(y_test)\n",
        "\tprediction=scaler.inverse_transform(prediction)\n",
        "\tmse_ram = mean_squared_error(y_test[:,10], prediction[:,10])\n",
        "\tmae_ram = mean_absolute_error(y_test[:,10], prediction[:,10])\n",
        "\tmse_cpu = mean_squared_error(y_test[:,4], prediction[:,4])\n",
        "\tmae_cpu = mean_absolute_error(y_test[:,4], prediction[:,4])\n",
        "\treturn mse, mae, mse_ram, mae_ram, mse_cpu, mae_cpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7amNv42UJOvg"
      },
      "source": [
        "def inference(model, X):\n",
        "\ti=random.randrange(len(X))\t\n",
        "\tstartS=time.time()\t\n",
        "\tprediction=model.predict(X[[i]])\t\n",
        "\ttimeS=time.time()-startS\n",
        "\ti=random.randrange(len(X)-100)\n",
        "\tstartB=time.time()\n",
        "\tprediction=model.predict(X[i:i+101])\n",
        "\ttimeB=time.time()-startB\n",
        "\treturn timeS, timeB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQQL-eUnDVGS"
      },
      "source": [
        "def data_preparation(datasetfile):\n",
        "  dataframe = pd.read_csv(datasetfile, engine='python')\n",
        "  dataset = dataframe.values\n",
        "  dataset = dataset.astype('float32')\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  scaler.fit(dataset)\n",
        "  dataset = scaler.transform(dataset)\n",
        "  return dataset, scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS32Jy8yKbWI"
      },
      "source": [
        "def remove_dupes(i, o):\n",
        "\tfor j in range(len(i)):\n",
        "\t\tfor k in range(len(i)):\n",
        "\t\t\tif (i[j]==i[k] and j!=k):\n",
        "\t\t\t\tif (o[j]>o[k]):\n",
        "\t\t\t\t\ti.pop(j)\n",
        "\t\t\t\t\to.pop(j)\n",
        "\t\t\t\t\treturn remove_dupes(i, o)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\ti.pop(k)\n",
        "\t\t\t\t\to.pop(k)\n",
        "\t\t\t\t\treturn remove_dupes(i, o)\n",
        "\treturn i, o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMBxqHFf-Cy5"
      },
      "source": [
        "def train_test(model, dataset, lookback):\n",
        "  if hasLSTM(model) or hasCONV1D(model):\n",
        "    dataset,datasetY=split_sequences(dataset,lookback,1) \n",
        "    datasetY=datasetY.reshape(datasetY.shape[0],datasetY.shape[2]) \n",
        "  else:\n",
        "    dataset,datasetY=split_sequences(dataset,1,1)  \n",
        "    dataset=dataset.reshape(dataset.shape[0],dataset.shape[2])\n",
        "    datasetY=datasetY.reshape(datasetY.shape[0],datasetY.shape[2])  \n",
        "  train_size = int(len(dataset) * 0.67)\n",
        "  test_size = len(dataset) - train_size\n",
        "  trainX, testX = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "  trainY, testY = datasetY[0:train_size,:], datasetY[train_size:len(dataset),:] \n",
        "  return trainX, testX, trainY, testY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVBLsmU9HOTA"
      },
      "source": [
        "def getOptimizer(x4, optimizer):\n",
        "  learning_rate = x4\n",
        "  if optimizer =='RMSprop':\n",
        "    optimizer = tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "  elif optimizer =='Adam':\n",
        "    optimizer = tensorflow.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  elif optimizer =='SGD':\n",
        "    optimizer = tensorflow.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "  elif optimizer =='Adagrad':\n",
        "    optimizer = tensorflow.keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
        "  elif optimizer =='Adadelta':\n",
        "    optimizer = tensorflow.keras.optimizers.Adadelta(learning_rate=learning_rate)\n",
        "  elif optimizer =='Adamax':\n",
        "    optimizer = tensorflow.keras.optimizers.Adamax(learning_rate=learning_rate)\n",
        "  else:\n",
        "    optimizer = tensorflow.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
        "\n",
        "  return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jQBMnLkrbjF"
      },
      "source": [
        "def build_model(activationFunction, optimizer):\n",
        "  global dataset, defineModel, lookback, typeOfModel, neurons, numOfLayers, dropout, noOfLSTM_CONV, pool_size\n",
        "  \n",
        "  if defineModel == False:\n",
        "    defineModel = True\n",
        "    models = ['with lstm', 'with conv']\n",
        "    typeOfModel = random.choice(models)\n",
        "\n",
        "  model = Sequential()\n",
        "  neurons = int(round(x0))\n",
        "  numOfLayers = int(round(x1))\n",
        "  dropout = truncate(x3,1)\n",
        "  lookback = int(round(x2))\n",
        "  optimization = getOptimizer(x4, optimizer)\n",
        "  noOfLSTM_CONV = int(round(x7))\n",
        "  pool_size = int(round(x8))\n",
        "  input_shape=(lookback,dataset.shape[1])\n",
        "  \n",
        "  if typeOfModel == 'with lstm': \n",
        "    if noOfLSTM_CONV == 1:\n",
        "      model.add(LSTM(neurons, input_shape=(lookback, dataset.shape[1]), activation=activationFunction, \n",
        "                     recurrent_activation='sigmoid', implementation=2))\n",
        "    elif noOfLSTM_CONV == 2:\n",
        "      model.add(LSTM(neurons, input_shape=(lookback, dataset.shape[1]), activation=activationFunction, recurrent_activation='sigmoid', implementation=2, return_sequences=True))\n",
        "      model.add(LSTM(neurons, activation=activationFunction, recurrent_activation='sigmoid', implementation=2)) \n",
        "  elif typeOfModel == 'with conv':\n",
        "    for i in range(1,noOfLSTM_CONV+1):\n",
        "      model.add(Conv1D(filters=neurons, kernel_size=2, padding='same', activation=activationFunction, input_shape=(lookback,dataset.shape[1])))\n",
        "      model.add(MaxPooling1D(pool_size=pool_size, padding='same'))\n",
        "      if i == 2:\n",
        "        model.add(Conv1D(filters=neurons, kernel_size=2, padding='same', activation=activationFunction))\n",
        "        model.add(MaxPooling1D(pool_size=pool_size, padding='same'))\n",
        "    model.add(Flatten())\n",
        "\n",
        "  units = neurons\n",
        "  for i in range(1,numOfLayers+1):\n",
        "      units = int(round(units/2))\n",
        "      if units > 0:    \n",
        "        model.add(Dense(units, activation=activationFunction))\n",
        "        model.add(Dropout(dropout))\n",
        "  model.add(Dense(dataset.shape[1]))\n",
        "\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=optimization,\n",
        "                metrics=['mae', 'mse'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWvEOzfXFCO2"
      },
      "source": [
        "global activationFunction, optimizer\n",
        "dimensions  = [Categorical(['tanh','sigmoid','linear','relu'], name='activationFunction'), #Search space, all parameters are nomminal\n",
        "              Categorical(['RMSprop', 'Adam', 'SGD', 'Adagrad', 'Adadelta', 'Adamax', 'Nadam'], name='optimizer')]\n",
        "defact = ['tanh','sigmoid','linear','relu']\n",
        "defopt =  ['RMSprop', 'Adam', 'SGD', 'Adagrad', 'Adadelta', 'Adamax', 'Nadam']\n",
        "default_parameters = [random.choice(defact), random.choice(defopt)]\n",
        "@use_named_args(dimensions=dimensions) #Combine Objective function with its search space\n",
        "def gp_minimize_opt_function(activationFunction, optimizer):\n",
        "\n",
        "  global loss, best_loss, dataset, lookback, trainings\n",
        "  epochs = int(round(x5))\n",
        "  batch_size = int(round(x6))\n",
        "  \n",
        "  graph = tensorflow.Graph()\n",
        "  with tensorflow.compat.v1.Session(graph=graph):\n",
        "\n",
        "    model = build_model(activationFunction, optimizer)\n",
        "    trainX, testX, trainY, testY = train_test(model, dataset, lookback)\n",
        "    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=0, callbacks=[EarlyStopper]) \n",
        "    loss, _, _ = model.evaluate(testX, testY, verbose=2)\n",
        "\n",
        "    if loss < best_loss:\n",
        "      best_loss = loss\n",
        "      model.save('Models/best')\n",
        "      lb=testX.shape[1]\n",
        "      print(lb, file=open('Models/lookback.txt','w'))\n",
        "     \n",
        "      with open(\"hyper.txt\", \"w\") as f:\n",
        "        var1 =\"neurons = \",str(neurons)\n",
        "        var2 = \"layers = \",str(numOfLayers)\n",
        "        var3 = \"lookback = \",str(lookback)\n",
        "        var4 =\"dropout = \",str(dropout)\n",
        "        var5 = \"learning rate = \",str(x4)\n",
        "        var6 = \"epochs = \",str(epochs)\n",
        "        var7 = \"batch_size = \",str(batch_size)\n",
        "        var8 = \"number of lstm/conv1d layers = \",str(noOfLSTM_CONV)\n",
        "        var9 = \"number of pool size = \",str(pool_size)\n",
        "        var10 = \"activation function = \",activationFunction\n",
        "        var11 = \"optimizer = \",optimizer\n",
        "        i = var1[0]+var1[1]+\"||\"+var2[0]+var2[1]+\"||\"+var3[0]+var3[1]+\"||\"+var4[0]+var4[1]+\"||\"+var5[0]+var5[1]+\"||\"+var6[0]+var6[1]+\"||\"+var7[0]+var7[1]+\"||\"+var8[0]+var8[1]+\"||\"+var9[0]+var9[1]+\"||\"+var10[0]+var10[1]+\"||\"+var11[0]+var11[1]   \n",
        "        f.write(i)\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBBBkqkLzurD"
      },
      "source": [
        "def pso_opt_fun(x):\n",
        "  global bn, bayes_inputs, bayes_results, bayes_time, bt, median_loss, best_swarm_loss, counter\n",
        "  global x0, x1, x2, x3, x4, x5, x6, x7, x8\n",
        "  x0 = x[0]\n",
        "  x1 = x[1]\n",
        "  x2 = x[2]\n",
        "  x3 = x[3]\n",
        "  x4 = x[4]\n",
        "  x5 = x[5]\n",
        "  x6 = x[6]\n",
        "  x7 = x[7]\n",
        "  x8 = x[8]\n",
        "  calls = 3\n",
        "  \n",
        "  bt=time.time()\n",
        "  if not bayes_inputs:\n",
        "\t  bayes = gp_minimize(func=gp_minimize_opt_function, dimensions=dimensions, acq_func='EI', \n",
        "                        n_calls=calls, n_random_starts=1, x0=default_parameters, model_queue_size=1)\n",
        "  else:\n",
        "\t  bayes = gp_minimize(func=gp_minimize_opt_function, dimensions=dimensions, acq_func='EI', \n",
        "                        n_calls=calls, n_random_starts=1, x0=bayes_inputs, y0=bayes_results, model_queue_size=1)\n",
        "  bayes_time=bayes_time+time.time()-bt\n",
        "  bn=bn+1\t\n",
        "  bayes_inputs.extend(bayes.x_iters)\n",
        "  bayes_results.extend(bayes.func_vals) \t\t\n",
        "  keep=calls*bn\n",
        "  bayes_inputs=bayes_inputs[-keep:]\n",
        "  bayes_results=bayes_results[-keep:]\n",
        "  bayes_inputs, bayes_results=remove_dupes(bayes_inputs, bayes_results)\n",
        "  error=min(bayes_results[-calls:])\n",
        "  \n",
        "  return error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzBNQIAULLsQ"
      },
      "source": [
        "EarlyStopper = EarlyStopping(patience=1, monitor='loss', mode='min')\n",
        "trainings=0\n",
        "best_loss = 1\n",
        "defineModel = False\n",
        "bn=0\n",
        "bayes_inputs = []\n",
        "bayes_results = []\n",
        "time_total=[]\n",
        "bayes_time_total=[]\n",
        "bt=0\n",
        "bayes_time=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxEcqSERJ7k5"
      },
      "source": [
        "#x0=neurons, x1=layers, x2=lookback\n",
        "#x3=dropout, x4=learning rate, x5=epochs\n",
        "#x6=batch_size, x7=number of lstm/conv1d layers, x8=pool_size\n",
        "lb = [1, 1, 1, 0, 0.001, 20, 1, 1, 1]\n",
        "ub = [128, 5, 5, 0.5, 0.2, 200, 1000, 2, 5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58GItjOOxFdG"
      },
      "source": [
        "mydatasetfile = 'https://docs.google.com/uc?export=download&id=1XhbneHtO6R5b2XD401J6kcfccMdl0c1f'\n",
        "dataset_name = mydatasetfile.split(\"/\")[-1]\n",
        "dataset, scaler = data_preparation(mydatasetfile)\n",
        "dataframe = pd.read_csv(mydatasetfile, engine='python')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtrtBhBIxGlg"
      },
      "source": [
        "start=time.time()\n",
        "xopt, fopt = pso(pso_opt_fun, lb, ub, swarmsize=10, omega=1, phip=0.5, phig=1.0, maxiter=10, minstep=2, minfunc=0.00015)\n",
        "mytime=time.time()-start\n",
        "\n",
        "print (\"Best position\"+str(xopt))\n",
        "print (\"Loss:\" + str(fopt))\n",
        "print(start, mytime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX5nOtAHxO_Y"
      },
      "source": [
        "mymodel=keras.models.load_model('Models/best')\n",
        "mydatasetfile = 'https://docs.google.com/uc?export=download&id=1XhbneHtO6R5b2XD401J6kcfccMdl0c1f'\n",
        "dataset_name = mydatasetfile.split(\"/\")[-1]\n",
        "dataset, scaler = data_preparation(mydatasetfile)\n",
        "text_file = open(\"Models/lookback.txt\", \"r\")\n",
        "lookback= int(text_file.read(1))\n",
        "if hasLSTM(mymodel) or hasCONV1D(mymodel):\n",
        "  dataset,datasetY=split_sequences(dataset,lookback,1)  \n",
        "  datasetY=datasetY.reshape(datasetY.shape[0],datasetY.shape[2]) \n",
        "else:\n",
        "  dataset,datasetY=split_sequences(dataset,1,1)  \n",
        "  dataset=dataset.reshape(dataset.shape[0],dataset.shape[2])\n",
        "  datasetY=datasetY.reshape(datasetY.shape[0],datasetY.shape[2])  \n",
        "train_size = int(len(dataset) * 0.67)\n",
        "test_size = len(dataset) - train_size\n",
        "trainX, testX = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "trainY, testY = datasetY[0:train_size,:], datasetY[train_size:len(dataset),:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAOZpcwzxdda"
      },
      "source": [
        "mse, mae, mse_ram, mae_ram, mse_cpu, mae_cpu = metrics(mymodel, testX, testY, scaler)\n",
        "infS, infB = inference(mymodel, testX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIsknhDhxbF6"
      },
      "source": [
        "mymodel.summary()\n",
        "file_o=open(\"hyper.txt\")   \n",
        "content=file_o.read()                 \n",
        "print(content)                       \n",
        "file_o.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CikgN1cI5R8"
      },
      "source": [
        "import math  \n",
        "print('PSO results using', dataset_name, 'as dataset')\n",
        "print('Best mse: %.6f\tmae: %.6f\trmse: %.6f\ttraining time: %.0f s' % (mse, mae, math.sqrt(mse), mytime))\n",
        "print('Single inference time: %.3f s\tbatch inference time: %.3f s ' % (infS, infB))\n",
        "print('RAM mse: %.6f\tRAM mae: %.6f\tRAM rmse: %.6f' % (mse_ram, mae_ram, math.sqrt(mse_ram)))\n",
        "print('CPU mse: %.6f\tCPU mae: %.6f\tCPU rmse: %.6f' % (mse_cpu, mae_cpu, math.sqrt(mse_cpu)))\n",
        "print('DeepLearning,',mytime,',',math.sqrt(mse),',',mae,',',infS,',',infB,',',math.sqrt(mse_cpu),',',mae_cpu,',',math.sqrt(mse_ram),',',mae_ram, file=open('results.csv','a'))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}